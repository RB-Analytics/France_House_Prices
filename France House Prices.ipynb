{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd0b1cbc199116ca0050f15b4a096ee5586221393ef42f89efdf0ba89fccf25361f",
   "display_name": "Python 3.7.6 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# PERSONAL PROJECT"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "In this notebook, I will investigate the data available to us by the Open Data initiative of the French goverment.  The focus will be on house prices within France, and more specifically within Paris.  \n",
    "\n",
    "Data is open source and is not bound by any commercial or usage constraints.  The data will be downloaded through the site, on this link: https://www.data.gouv.fr/fr/datasets/demandes-de-valeurs-foncieres/\n",
    "\n",
    "The exploratory visualisations will be available on https://public.tableau.com/profile/rodrigo.baptista\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Importing libraries\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import sklearn\n",
    "import chardet\n",
    "import os, shutil\n",
    "\n",
    "\n",
    "#webscraping\n",
    "from lxml import html, etree\n",
    "import requests\n",
    "import urllib\n",
    "\n",
    "#data science\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import dtale\n",
    "\n",
    "#visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check packages versions\n",
    "print(\"\\n# check packages versions\\n\")\n",
    "\n",
    "print(\"python: \" + sys.version)\n",
    "\n",
    "print(\"sklearn: \" + sklearn.__version__)"
   ]
  },
  {
   "source": [
    "## Useful Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_filePath (filePath_varString, filePath):\n",
    "    \"\"\"\n",
    "    Checks if the file exists and prints the value, else exits.\n",
    "    \"\"\"\n",
    "    if not filePath.exists():\n",
    "        print(\"ERROR, \" + filePath_varString + \" not found!!!\")\n",
    "        sys.exit()\n",
    "    print(filePath_varString + \": \" + filePath._str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url: str, dest_folder: str, filename: str):\n",
    "    if not os.path.exists(dest_folder):\n",
    "        os.makedirs(dest_folder)  # create folder if it does not exist\n",
    "\n",
    "    #filename = url.split('/')[-1].replace(\" \", \"_\")  # be careful with file names\n",
    "    file_path = os.path.join(dest_folder, filename)\n",
    "\n",
    "    r = requests.get(url, stream=True)\n",
    "    if r.ok:\n",
    "        print(\"saving to\", os.path.abspath(file_path))\n",
    "        with open(file_path, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024 * 8):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    f.flush()\n",
    "                    os.fsync(f.fileno())\n",
    "    else:  # HTTP status code 4XX/5XX\n",
    "        print(\"Download failed: status code {}\\n{}\".format(r.status_code, r.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_encoding (filePath):\n",
    "    \"\"\"\n",
    "    Returns the encoding of the file\n",
    "    \"\"\"\n",
    "    with open(filePath, mode='rb') as f:\n",
    "        # result = chardet.detect(f.read(100000))\n",
    "        detector = chardet.universaldetector.UniversalDetector()\n",
    "        count = 0\n",
    "        for line in f.readlines():\n",
    "            detector.feed(line)\n",
    "            count += 1\n",
    "            if count == 1000: break\n",
    "            if detector.done: break\n",
    "        detector.close()\n",
    "        # print(detector.result)\\n\",\n",
    "    # print(filePath._str, \"\\n Encoding: \", result['encoding'])\n",
    "\n",
    "    # return result['encoding']\n",
    "    return detector.result['encoding']"
   ]
  },
  {
   "source": [
    "## Set up directories"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Data Folder Paths depending on User\n",
    "print(\"\\n# Set Data Folder Paths for project\\n\")\n",
    "\n",
    "# Set Workdir folder where projects are developed\n",
    "workdir_folder = Path(\"/workspaces/hsbc-gpmo-account\")\n",
    "if not workdir_folder.exists():\n",
    "    workdir_folder = Path(\"C:\\\\Users\\\\rodri\\\\OneDrive\\\\Documents\\\\Data Science Projects - Personal\\\\opendata-france\\\\House Prices Paris - Datasources\")\n",
    "check_filePath(\"workdir_folder\", workdir_folder)\n",
    "\n",
    "\n",
    "# Set Source folders where data sources for project are stored\n",
    "sources_folder = Path(\"/workspaces/hsbc-gpmo-account\")\n",
    "if not sources_folder.exists():\n",
    "    sources_folder = Path(\"C:\\\\Users\\\\rodri\\\\OneDrive\\\\Documents\\\\Data Science Projects - Personal\\\\opendata-france\\\\House Prices Paris - Datasources\")\n",
    "check_filePath(\"sources_folder\", sources_folder)"
   ]
  },
  {
   "source": [
    "## Scraping data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "To keep this notebook up to date and relevant, the data download will be automated to fetch directly from the site. This ensures I'm able to have access to any new datasets that are updated to the website.\n",
    "\n",
    "\n",
    "Looking through the datasets, their names are structured and consistent.  At the time of writing this, the naming convention follows:\n",
    "\n",
    "    valeursfoncieres-year-extra\n",
    "\n",
    "The extra component is included for cases where the dataset does not cover a full year.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the original webpage html content\n",
    "webpageLink = 'https://www.data.gouv.fr/fr/datasets/demandes-de-valeurs-foncieres/'\n",
    "page = requests.get(webpageLink)\n",
    "# convert the data received into searchable HTML\n",
    "extractedHtml = html.fromstring(page.content)\n",
    "# use an XPath query to find the datasets link \n",
    "# to do this, we search for all download button links and get datasetlink\n",
    "dataSrc = extractedHtml.xpath('//div[@class=\"resource-card-actions btn-toolbar\"]/a/@href')  \n",
    "# I'll assume the documentation for the data set will be limited to the 4 links already located at the bottom of the website plus 4 download links for misc resources. So, the relevant files to keep are all but the last 8\n",
    "dataSrc\n",
    "dataSrc2=dataSrc[:-8]\n",
    "dataSrc2.reverse()"
   ]
  },
  {
   "source": [
    "Load and concatenate data\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It's now ready to download the files and combine them\n",
    "# we will only download the files if they don't already exist:\n",
    "my_file= Path(\"opendata-france\\\\House Prices Paris - Datasources/valuers_foncieres_combined\")\n",
    "#load dataset faster by filtering at source\n",
    "req_columns=[\"Year_Price\",\"No disposition\", \"Date mutation\", \"Nature mutation\", \"Valeur fonciere\", \"Code voie\", \"Voie\", \"Code postal\", \"Commune\", \"Code departement\", \"Code commune\", \"Section\", \"No plan\", \"Nombre de lots\", \"Nature culture\", \"Surface terrain\"]\n",
    "\n",
    "if my_file.is_file():\n",
    "    source_file= sources_folder/str(\"valuers_foncieres_combined\")\n",
    "    #option 1 - faster - use this normally\n",
    "    #prices = pd.read_csv(source_file,sep= ',',encoding=detect_encoding(source_file), usecols=req_columns)\n",
    "    #option 2\n",
    "    prices = pd.read_csv(source_file,sep= ',',encoding=detect_encoding(source_file), nrows=1000)\n",
    "    \n",
    "    print(\"Loaded Combined prices file!\")\n",
    "else:\n",
    "    \n",
    "    len(dataSrc2)\n",
    "    start_year=2015\n",
    "    i=0\n",
    "\n",
    "    \n",
    "\n",
    "    #for each file, download and name it accordingly\n",
    "    for link in dataSrc2:\n",
    "        download(link, dest_folder=\"opendata-france\\\\House Prices Paris - Datasources\", filename=\"valuers_foncieres_\"+str(start_year+i))\n",
    "        i=i+1\n",
    "\n",
    "    prices=pd.DataFrame()\n",
    "\n",
    "    # create master dataset\n",
    "    for i in range(len(dataSrc2)):\n",
    "        source_file= sources_folder/str(\"valuers_foncieres_\"+str(start_year+i))\n",
    "        #load dataset\n",
    "        df = pd.read_csv(source_file,sep= '|',encoding=detect_encoding(source_file))\n",
    "        df[\"Year_Price\"]=str(start_year+i)\n",
    "        prices=prices.append(df)\n",
    "\n",
    "        # df_2015 = pd.read_csv(sources_folder/str(\"valuers_foncieres_\"+str(2015)),sep= '|')\n",
    "        # df=df_2015\n",
    "        # df_2016 = pd.read_csv(sources_folder/str(\"valuers_foncieres_\"+str(2016)),sep= '|')\n",
    "        # df=df.append(df_2016)\n",
    "        # df_2017 = pd.read_csv(sources_folder/str(\"valuers_foncieres_\"+str(2017)),sep= '|')\n",
    "        # df=df.append(df_2017)\n",
    "        # df_2018 = pd.read_csv(sources_folder/str(\"valuers_foncieres_\"+str(2018)),sep= '|')\n",
    "        # df=df.append(df_2018)\n",
    "        # df_2019 = pd.read_csv(sources_folder/str(\"valuers_foncieres_\"+str(2019)),sep= '|')\n",
    "        # df=df.append(df_2019)\n",
    "        # df_2020 = pd.read_csv(sources_folder/str(\"valuers_foncieres_\"+str(2020)),sep= '|')\n",
    "        # df=df.append(df_2020)\n",
    "    \n",
    "    # delete unnecessary files\n",
    "    folder = sources_folder\n",
    "    for filename in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, filename)\n",
    "        print(\"Deleting existing files...\")\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "\n",
    "    #save dataframe\n",
    "    prices.to_csv(sources_folder / \"valuers_foncieres_combined\",index=True)\n",
    "    print(\"Saved Combined prices file!\")\n",
    "    "
   ]
  },
  {
   "source": [
    "# Create Master Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all column which have more than 60% missing values\n",
    "# ratio = (prices.isna().sum()/len(prices))>0.4\n",
    "# prices_cleaned=prices[prices.columns[ratio==False]]\n",
    "prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_cleaned.head()"
   ]
  },
  {
   "source": [
    "# Exploratory Data Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "I will start by investigating the distributions of the variables I have on the dataset.  The first step is to identify which variables are categorical and which are numeric. I can then investigate if I need to do any data conversions in between.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some data cleaning\n",
    "prices_cleaned[\"Valeur fonciere\"]=prices_cleaned[\"Valeur fonciere\"].str.replace(',','.').astype(np.float64)\n",
    "prices_cleaned=prices_cleaned[prices_cleaned[\"Valeur fonciere\"]<100000000]"
   ]
  }
 ]
}